{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lyrics_Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdrS4X7Z9Qw0",
        "colab_type": "text"
      },
      "source": [
        "# **Lyrics Generation using RNN**\n",
        "### (can be generalised as coherent / meaningful text generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTaAxneu_OzW",
        "colab_type": "text"
      },
      "source": [
        "#### **Importing TensorFlow and other libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp88IY6N9UJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m_2sMc4_owO",
        "colab_type": "text"
      },
      "source": [
        "#### **Specifying the path of the text file to process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZYe710ju3tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = 'lyrics_dataset.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO-5lBnNAFTE",
        "colab_type": "text"
      },
      "source": [
        "#### **Opening the text file in read mode**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1k2DRXZvG9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Opening the text file in read mode and standard encoding it\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHajuqtVvaOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A look at the first 250 characters in text\n",
        "print(text[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AExcxzvKvd1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDkpbEEmA2sx",
        "colab_type": "text"
      },
      "source": [
        "#### **Preprocessing of text i.e from strings to numerical representation**\n",
        "##### Creating lookup-tables for char->num and num->char"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpFuAN8XvlRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y22qwVJlwOvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('{ ===========>')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n==========>}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5-xHoW6wSgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show how the first 20 characters from the text are mapped to integers\n",
        "print ('{} ==> characters mapped to int ==> {}'.format(repr(text[:20]), text_as_int[:20]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvv5ug90CPPg",
        "colab_type": "text"
      },
      "source": [
        "#### **Creating training examples and targets**\n",
        "Broke the text into chunks of seq_length + 1 , if text is => \"Alright\" \n",
        "\n",
        "Then input sequence becomes => \"Alrigh\"\n",
        "\n",
        "The output sequence becomes => \"lright\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCl-BN0rwXNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()] , end = \"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr-JDdQm3Ewd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using batch method converted individual characters to sequences of desired size\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62obB6BPEk-6",
        "colab_type": "text"
      },
      "source": [
        "#### **Mapping function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E1qhqdT3RN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSMOgVwJ3Wp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLEdn7tJGcEm",
        "colab_type": "text"
      },
      "source": [
        "#### **Creating training batches for splitting text into manageable sequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18mKD_EI3Z_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmPdZBGG3lFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units \n",
        "rnn_units = 1500 # keep between (1024 -> 1800) for best results "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgu__rqDHI9p",
        "colab_type": "text"
      },
      "source": [
        "### **Buliding the Model**\n",
        "#### **Four layers were used :**\n",
        "######  **1. Embedding layer :** The input layer. A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions\n",
        "######  **2. GRU layer :** A type of RNN with size units=rnn_units (LSTM could also be used here.)\n",
        "###### **3. Dense layer :** The output layer, with vocab_size outputs and 'RELU' as the activation fuction \n",
        "###### **4. Dropout layer :** Benifits regularisation and prevents overfitting  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t64ZXPaH4AfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "                               \n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "  \n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "    tf.keras.layers.Dense(vocab_size,activation='relu'),\n",
        "    \n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7YNdwZA4HW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zMgA-w34Kcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPFvpo6xJrom",
        "colab_type": "text"
      },
      "source": [
        "#### **Model Summary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_FQFXvb4Nin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtJu9jF74Rbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l21C3EHw4XPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYN-oiic4bXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcKIsigsJ1cE",
        "colab_type": "text"
      },
      "source": [
        "#### **Attaching an optimizer, and a loss function**\n",
        "###### **tf.keras.losses.sparse_categorical_crossentropy** loss function works in this case because it is applied across the last dimension of the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TaUVaap4gI8",
        "colab_type": "code",
        "outputId": "69de9850-7375-435e-98fb-c6d658222bc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 98)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.5856647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "himU-fInKRYp",
        "colab_type": "text"
      },
      "source": [
        "#### **Adam optimiser gives the best result hands down**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plUUV_mT4nUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqjrgQzOKcnU",
        "colab_type": "text"
      },
      "source": [
        "#### **Configuring the checkpoints , ensuring that the checkpoints are saved during training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXDJ7-rR4rTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an6jFLaGKvXJ",
        "colab_type": "text"
      },
      "source": [
        "#### **Rounds of training -> EPOCHS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MT0yILo4ugb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure it according to the loss you get at the end.\n",
        "# Ensure that the loss is between 1.4 to 1.1 for best meaningful text generation ensuring that it is always new lyrics/text , NOT same as in the training set\n",
        "# If the loss becomes less than 1 , then a lot of same text would be generated\n",
        "# Obviously we don't want the same , but something new\n",
        "\n",
        "EPOCHS=25 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqBYt65U41vA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k32Wp91lL9Q-",
        "colab_type": "text"
      },
      "source": [
        "#### **Generating text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK8pA3zK44s3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxZKKZhX5fsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5omajupS5ja6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ6O9y6HMLjI",
        "colab_type": "text"
      },
      "source": [
        "#### **Prediction Loop : the code block that generates the text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g73reOa05mcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, chars_to_generate , temp , start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = chars_to_generate\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vSQLUizOGGe",
        "colab_type": "text"
      },
      "source": [
        "#### **Generation (finally :))**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHOx_XJb5yQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import arange\n",
        "\n",
        "# Number of characters to generate (keep between 250 to 500)\n",
        "chars_to_generate = 500 \n",
        "\n",
        "# Printing the generated text\n",
        "# Temperature 1.0 gives the craziest output and 0.1 gives the lowest varience\n",
        "# Keeping the temperature 0.35 gives best meaningful / coherent text.\n",
        "\n",
        "# Give the seed string as the first word of generate text\n",
        "print(generate_text(model , chars_to_generate , 0.35 , start_string=u\"Love \"))\n",
        "\n",
        "# Uncomment below to check the variences ==>\n",
        "\n",
        "# for i in arange(0.1,1.1,0.1):\n",
        "#   print(\"==============\")\n",
        "#   print(\"FOR TEMP : {} \".format(i))\n",
        "#   print(\"==============\")\n",
        "#   print(generate_text(model , chars_to_generate , i , start_string=u\"Love \"))\n",
        "#   print()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}